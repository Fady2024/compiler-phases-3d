export interface CompilerPhase {
  id: string;
  name: string;
  subtitle: string;
  description: string;
  color: string;
  position: [number, number, number];
  details: string[];
  inputCode?: string;
  outputCode?: string;
  technicalDetails?: {
    what: string;
    why: string;
    how: string;
  };
}

export const compilerPhases: CompilerPhase[] = [
  {
    id: "source",
    name: "Source Code",
    subtitle: "Input Program",
    description: "The original high-level program written by the developer.",
    color: "#22c55e",
    position: [-4, 2.5, 0],
    details: [
      "Written in High-Level Language",
      "Human-readable syntax",
      "Describes program logic",
      "Starting point of compilation",
      "Contains preprocessor directives",
      "Defines algorithms & data structures"
    ],
    inputCode: `// Developer writes:
int main() {
  int x = 5 + 10;
  return x;
}`,
    outputCode: `// Passed to
// Lexical Analyzer`,
    technicalDetails: {
      what: "The Source Code phase represents the initial input provided by the programmer. It is the raw text written in a high-level programming language (like C, C++, Java, or TypeScript) that contains the logic and instructions for the computer to execute.",
      why: "Computers operate on binary machine code (0s and 1s), which is extremely difficult for humans to read and write. High-level languages allow developers to write code using human-readable syntax and abstractions, making software development more efficient and less error-prone. The source code is the bridge between human logic and machine execution.",
      how: "The developer writes code using a text editor or IDE. This code is saved as a text file (e.g., .c, .ts, .py). The compiler then reads this file as a stream of characters. At this stage, the compiler doesn't understand the code's meaning; it just sees a sequence of letters, numbers, and symbols waiting to be processed."
    }
  },
  {
    id: "lexical",
    name: "Lexical Analysis",
    subtitle: "Tokenization",
    description: "Converts the sequence of characters into a sequence of tokens.",
    color: "#3b82f6",
    position: [-4, 0.5, 0],
    details: [
      "Scans character stream",
      "Recognizes lexical patterns",
      "Creates tokens (lexemes)",
      "Filters whitespace & comments",
      "Reports invalid characters",
      "Tracks line numbers for errors"
    ],
    inputCode: `int x = 5 + 10;`,
    outputCode: `<KEYWORD, int>
<ID, x>
<SEMICOLON, ;>`,
    technicalDetails: {
      what: "Lexical Analysis (or Scanning) is the first phase of compilation. It groups the raw stream of characters from the source code into meaningful sequences called 'tokens' or 'lexemes'. It acts as the vocabulary builder for the compiler.",
      why: "Processing individual characters is inefficient and complex for the subsequent parser. By grouping characters into tokens (like keywords, identifiers, numbers, operators), the compiler reduces the complexity of the input. It also removes unnecessary elements like whitespace and comments, which have no meaning for the code's execution.",
      how: "The Lexer scans the source code character by character. It uses a set of rules (often defined by Regular Expressions) to recognize patterns. When it matches a pattern (e.g., 'int'), it creates a token (e.g., <KEYWORD, int>). If it encounters something it doesn't recognize, it reports a lexical error. The output is a stream of tokens that serves as input for the next phase."
    }
  },
  {
    id: "syntax",
    name: "Syntax Analysis",
    subtitle: "Parsing",
    description: "Analyzes the grammatical structure and builds the Abstract Syntax Tree (AST).",
    color: "#ec4899",
    position: [0, 0.5, 0],
    details: [
      "Checks Grammar Rules",
      "Builds Parse Tree / AST",
      "Verifies structure",
      "Uses Context-Free Grammars",
      "Detects syntax errors",
      "Handles operator precedence"
    ],
    inputCode: `Tokens:
<KEYWORD,int> <ID,x> <OP,=>
<NUM,5> <OP,+> <NUM,10>
<SEMICOLON,;>`,
    outputCode: `AST (Parse Tree):
       [=]
      /   \\
   [x]    [+]
         /   \\
       [5]   [10]`,
    technicalDetails: {
      what: "Syntax Analysis (or Parsing) checks if the sequence of tokens generated by the lexer conforms to the grammatical rules of the programming language. It's like checking if a sentence in English has the correct structure (Subject-Verb-Object).",
      why: "Even if all words (tokens) are valid, their arrangement might not make sense (e.g., '5 = x int;'). Syntax analysis ensures the code follows the language's grammar structure. It builds a hierarchical structure called a Parse Tree or Abstract Syntax Tree (AST) that represents the relationships between tokens.",
      how: "The Parser accepts the stream of tokens and tries to build a tree structure based on a formal grammar (Context-Free Grammar). It checks rules like 'an assignment statement must have a variable on the left and an expression on the right'. If the tokens don't fit the rules, it reports a syntax error. The AST produced (shown in the output) captures the nesting and precedence of operations."
    }
  },
  {
    id: "semantic",
    name: "Semantic Analysis",
    subtitle: "Type Checking",
    description: "Adds semantic meaning to AST: type checking, scope validation, and symbol resolution.",
    color: "#f97316",
    position: [4, 0.5, 0],
    details: [
      "Annotates AST with types",
      "Type Checking & Validation",
      "Symbol Table lookup",
      "Scope & Declaration checks",
      "Ensures function argument matching",
      "Detects type mismatches"
    ],
    inputCode: `AST from Parser:
    [=]
   /   \\
 [x]   [+]
      /   \\
    [5]   [10]`,
    outputCode: `Annotated AST:
    [=]:int
   /        \\
[x]:int   [+]:int
         /      \\
    [5]:int  [10]:int

Symbol Table: x→int
✓ All type-safe`,
    technicalDetails: {
      what: "Semantic Analysis validates the meaning of the well-formed parse tree. While the parser ensures the grammar is correct, the semantic analyzer ensures the code makes sense logically. For example, it checks if you are trying to add a number to a string, or using a variable that hasn't been declared.",
      why: "Grammatically correct code can still be meaningless or crash the machine. Adding 'true + 5' might be syntactically valid in some grammars but semantically invalid. This phase catches logic errors like type mismatches, undeclared variables, and scope violations before code generation begins, ensuring runtime safety.",
      how: "This phase traverses the AST and gathers information about identifiers (variables, functions) in a Symbol Table. It performs checks like: Is 'x' declared? Is 'x' an integer? Are we assigning an integer to an integer variable? It augments the AST with type information (e.g., annotating nodes with ':int'). If any rule is violated (like a type mismatch), a semantic error is reported."
    }
  },
  {
    id: "intermediate",
    name: "Intermediate Code",
    subtitle: "IR Generation",
    description: "Generates a machine-independent intermediate representation.",
    color: "#8b5cf6",
    position: [4, -1.5, 0],
    details: [
      "Three-Address Code (TAC)",
      "Static Single Assignment (SSA)",
      "LLVM IR Representation",
      "Machine Independent",
      "Preserves control flow",
      "Enables cross-platform support"
    ],
    inputCode: `From Semantic:
int x = 5 + 10;`,
    outputCode: `--- TAC (3-Address Code) ---
t1 = 5
t2 = 10
t3 = t1 + t2
x = t3

--- SSA (Static Single Assignment) ---
x1 = 5
x2 = 10
x3 = x1 + x2

--- LLVM IR ---
%x = alloca i32
store i32 5, i32* %t1
store i32 10, i32* %t2
%t3 = add i32 5, 10
store i32 %t3, i32* %x`,
    technicalDetails: {
      what: "Intermediate Code Generation creates an internal representation (IR) of the source code. This IR is simpler than the high-level source code but more abstract than machine code. It acts as a middle ground.",
      why: "Using an IR allows the compiler to be machine-independent. The same front-end (source to IR) can be used for different target machines (IR to machine code). It also simplifies optimization, as optimizers can work on this standard IR without worrying about the specifics of the source language or the target hardware.",
      how: "The compiler traverses the annotated AST and translates it into a linear sequence of simple instructions. Common formats include Three-Address Code (TAC), where each instruction has at most three operands (e.g., t1 = a + b), or LLVM IR. Temporary variables (t1, t2...) are generated to hold intermediate results of complex expressions."
    }
  },
  {
    id: "optimization",
    name: "Code Optimization",
    subtitle: "Optimization",
    description: "Improves code efficiency by reducing size and execution time.",
    color: "#ef4444",
    position: [0, -1.5, 0],
    details: [
      "Constant Folding",
      "Dead Code Elimination",
      "Loop Optimization",
      "Reduces resource usage",
      "Inline function expansion",
      "Common subexpression elimination"
    ],
    inputCode: `Before Optimization:
t1 = 5
t2 = 10
t3 = t1 + t2
x = t3`,
    outputCode: `After Optimization:
// Constant folding applied
x = 15

(Eliminated: t1, t2, t3)`,
    technicalDetails: {
      what: "Code Optimization attempts to improve the intermediate code so that the final machine code runs faster and consumes fewer resources (memory, power), without changing the program's behavior.",
      why: "Direct translation from high-level code often produces redundant or inefficient instructions. Optimization is crucial for performance. It eliminates unnecessary calculations, reduces memory access, and streamlines loops, making the resulting software efficient.",
      how: "The optimizer analyzes the IR to find patterns that can be improved. Examples shown here: 'Constant Folding' (calculating 5+10 at compile time to get 15) and 'Dead Code Elimination' (removing t1, t2, t3 since they are no longer needed). Other techniques include loop unrolling and common subexpression elimination. It effectively rewrites the IR into a more efficient version."
    }
  },
  {
    id: "codegen",
    name: "Code Generation",
    subtitle: "Assembly",
    description: "Translates the optimized IR into target-specific assembly code.",
    color: "#3b82f6",
    position: [-4, -1.5, 0],
    details: [
      "Instruction Selection",
      "Register Allocation",
      "Memory Management",
      "Target-specific (e.g., x86, ARM)",
      "Handles calling conventions",
      "Optimizes instruction scheduling"
    ],
    inputCode: `Optimized IR:
x = 15`,
    outputCode: `Assembly (ARM):
MOV R0, #15    ; Load 15
STR R0, [x]    ; Store to x`,
    technicalDetails: {
      what: "Code Generation is the backend phase where the optimized intermediate code is translated into the target machine's specific assembly language or machine code (e.g., x86 for Intel CPUs, ARM for mobile devices).",
      why: "The CPU cannot understand IR; it only understands its own specific instruction set architecture (ISA). This phase bridges the gap between the abstract software logic and the concrete hardware reality. It must choose the best machine instructions to implement the IR operations.",
      how: "The code generator maps IR instructions to machine instructions. It handles 'Register Allocation' (deciding which variables go into the limited, fast CPU registers vs. slow RAM) and 'Instruction Selection' (choosing the best assembly opcode). The output is assembly code that is specific to the target hardware architecture."
    }
  },
  {
    id: "executable",
    name: "Executable Code",
    subtitle: "Binary",
    description: "The final binary file ready to be executed by the machine.",
    color: "#22c55e",
    position: [-4, -3.5, 0],
    details: [
      "Binary Format (ELF/PE)",
      "Linker Resolution",
      "Machine Code",
      "Ready to Run",
      "Includes library dependencies",
      "OS-specific headers & entry"
    ],
    inputCode: `Assembly:
MOV R0, #15
STR R0, [x]`,
    outputCode: `Binary (ELF):
7F 45 4C 46 02 01 01 00
E3 A0 00 0F E5 9F 00 04
...
(Executable machine code)`,
    technicalDetails: {
      what: "The Executable Code phase involves the final steps of assembling and linking. The assembly code is converted into binary machine code (0s and 1s), and necessary library code is linked to create a standalone executable file.",
      why: "A program often consists of multiple files and relies on external libraries (like 'printf' or math functions). The CPU needs a single, complete binary file to run. This phase combines all these pieces into one coherent package that the Operating System can load and execute.",
      how: "An 'Assembler' converts the assembly text into binary object code. A 'Linker' then takes this object code and combines it with code from system libraries. It resolves memory addresses and placeholders, arranging everything into a specific file format (like ELF for Linux or PE for Windows). The result is a binary file that you can double-click to run."
    }
  }
];

export const connections = [
  { from: "source", to: "lexical" },
  { from: "lexical", to: "syntax" },
  { from: "syntax", to: "semantic" },
  { from: "semantic", to: "intermediate" },
  { from: "intermediate", to: "optimization" },
  { from: "optimization", to: "codegen" },
  { from: "codegen", to: "executable" },
];
